{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading traind and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "print(f\"Train shape = {train.shape}\\nTest shape = {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "print(f\"NULL values\\n\\nTrain\\n{train.isnull().sum()}\\n\\nTest\\n{test.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train describe\n",
    "train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test describe\n",
    "test.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disaster or not frequency\n",
    "target_counts=train[\"target\"].value_counts()\n",
    "print(target_counts)\n",
    "\n",
    "p1= round(100*(target_counts[1]/sum(target_counts)),2)\n",
    "p2= round(100-p1,2)\n",
    "print(f\"\\nPercentage of Disaster Tweets: {p1}%\\nPercentage of Non Disaster Tweets: {p2}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train[\"target\"])\n",
    "plt.title(\"Disaster (1) or not (0) barplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum length tweet\n",
    "print(\"Train max length tweet:\",train[\"text\"].apply(len).max())\n",
    "print(\"Test max length tweet:\",test[\"text\"].apply(len).max(),\"\\n\")\n",
    "\n",
    "#minimum length tweet\n",
    "print(\"Train min length tweet:\",train[\"text\"].apply(len).min())\n",
    "print(\"Test min length tweet:\",test[\"text\"].apply(len).min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating train into 2 datasets to get value_counts of keywords\n",
    "print(\"Disaster Keywords\\n\\n\",train[train[\"target\"]==1][\"keyword\"].value_counts()[:10],\"\\n\")\n",
    "print(\"Non Disaster Keywords\\n\\n\",train[train[\"target\"]==0][\"keyword\"].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#barplots of keywords\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Most commom keywords of disaster tweets\")\n",
    "sns.barplot(x=train[train[\"target\"]==1][\"keyword\"].value_counts()[:10],\\\n",
    "              y=train[train[\"target\"]==1][\"keyword\"].value_counts()[:10].index,orient=\"h\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Most commom keywords of non disaster tweets\")\n",
    "sns.barplot(x=train[train[\"target\"]==0][\"keyword\"].value_counts()[:10],\\\n",
    "              y=train[train[\"target\"]==0][\"keyword\"].value_counts()[:10].index,orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating train into 2 datasets to get value_counts of locations\n",
    "print(\"Disaster Keywords\\n\\n\",train[train[\"target\"]==1][\"location\"].value_counts()[:10],\"\\n\")\n",
    "print(\"Non Disaster Keywords\\n\\n\",train[train[\"target\"]==0][\"location\"].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#barplots of locations\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Most commom locations of disaster tweets\")\n",
    "sns.barplot(x=train[train[\"target\"]==1][\"location\"].value_counts()[:10],\\\n",
    "              y=train[train[\"target\"]==1][\"location\"].value_counts()[:10].index,orient=\"h\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Most commom locations of non disaster tweets\")\n",
    "sns.barplot(x=train[train[\"target\"]==0][\"location\"].value_counts()[:10],\\\n",
    "              y=train[train[\"target\"]==0][\"location\"].value_counts()[:10].index,orient=\"h\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for line break in tweets\n",
    "train['text'].str.contains(\"\\n\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for links in tweets\n",
    "train['text'].str.contains(\"https\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "stemmer_snowball = SnowballStemmer(\"english\")\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    link = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return link.sub(r\"\",tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing links\n",
    "train[\"text\"] = train[\"text\"].apply(remove_links)\n",
    "test[\"text\"] = test[\"text\"].apply(remove_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming all tweets into lower case letters\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: x.lower())\n",
    "test[\"text\"] = test[\"text\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokenizer.fit_on_texts(train[\"text\"])\n",
    "sequences_train=tokenizer.texts_to_sequences(train[\"text\"]) \n",
    "train[\"text\"] = tokenizer.sequences_to_texts(sequences_train)\n",
    "\n",
    "sequences_test=tokenizer.texts_to_sequences(test[\"text\"]) \n",
    "test[\"text\"] = tokenizer.sequences_to_texts(sequences_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation with ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "vec_rf = Pipeline([(\"Tfid\",TfidfVectorizer()),(\"rfclass\",RandomForestClassifier())])\n",
    "vec_xg = Pipeline([(\"Tfid\",TfidfVectorizer()),(\"xgboost\",XGBClassifier())])\n",
    "vec_nb = Pipeline([(\"Tfidf\",TfidfVectorizer()),(\"clf\",MultinomialNB())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[\"text\"]\n",
    "y = train[\"target\"]\n",
    "results_nb = []\n",
    "results_xg = []\n",
    "results_rf = []\n",
    "results_lstm = []\n",
    "\n",
    "kf = RepeatedKFold(n_splits=2,n_repeats=5,random_state=10)\n",
    "\n",
    "for train_l, valid_l in kf.split(X):\n",
    "    X_train, X_valid = X.iloc[train_l],X.iloc[valid_l]\n",
    "    y_train, y_valid = y.iloc[train_l],y.iloc[valid_l]\n",
    "    \n",
    "    vec_nb.fit(X_train,y_train)\n",
    "    p = vec_nb.predict(X_valid)\n",
    "    results_nb.append(f1_score(p,y_valid))\n",
    "    \n",
    "    vec_xg.fit(X_train,y_train)\n",
    "    p = vec_xg.predict(X_valid)\n",
    "    results_xg.append(f1_score(p,y_valid))\n",
    "    \n",
    "    vec_rf.fit(X_train,y_train)\n",
    "    p = vec_rf.predict(X_valid)\n",
    "    results_rf.append(f1_score(p,y_valid))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Random Forest = \",np.mean(results_rf))\n",
    "print(\"XGB = \",np.mean(results_xg))\n",
    "print(\"Naive Baiyes = \",np.mean(results_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE= len(tokenizer.word_index)+1\n",
    "EPOCHS=30\n",
    "MAXLEN=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(sequences_train,maxlen=MAXLEN, padding='post', truncating='post')\n",
    "test_pad = pad_sequences(sequences_test,maxlen=MAXLEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE,32,input_length=MAXLEN),\n",
    "    tf.keras.layers.SpatialDropout1D(0.2),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dense(1,activation=\"relu\")\n",
    "])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    verbose=1,\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"Adam\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,batch_size=64,epochs=EPOCHS,\\\n",
    "                    validation_data = (X_val,y_val),callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM submission\n",
    "y_test = model.predict(test_pad)\n",
    "p = [int(i>0.5) for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.Series(p,index=test[\"id\"],name=\"target\")\n",
    "sub.to_csv(\"submission.csv\",header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
