#plot for errors/kvalues
kValues <- 1:20
mError <- data.frame(kValues, mError)
ggplot(mError, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
for(i in 1:40) {
predicted.species <- knn(train.iris[1:4],test.iris[1:4],train.iris$Species,k=i)
mError[i] <- mean(predicted.species!=test.iris$Species)
}
#plot for errors/kvalues
kValues <- 1:40
mError <- data.frame(kValues, mError)
ggplot(mError, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
#choosing k value
mError <- NULL
predicted.species <- NULL
for(i in 1:40) {
predicted.species <- knn(train.iris[1:4],test.iris[1:4],train.iris$Species,k=i)
mError[i] <- mean(predicted.species!=test.iris$Species)
}
#plot for errors/kvalues
kValues <- 1:40
mError <- data.frame(kValues, mError)
ggplot(mError, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
#choosing k value
mError <- NULL
predicted.species <- NULL
for(i in 1:10) {
predicted.species <- knn(train.iris[1:4],test.iris[1:4],train.iris$Species,k=i)
mError[i] <- mean(predicted.species!=test.iris$Species)
}
#plot for errors/kvalues
kValues <- 1:10
mError <- data.frame(kValues, mError)
ggplot(mError, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
#as we can see 3 is the optimum solution
#accuracy
1-mError[3]
m.Error <- data.frame(kValues, mError)
ggplot(m.Error, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
#as we can see 3 is the optimum solution
#accuracy
1-m.Error$mError[3]
m.Error$mError
print(paste("Accuracy = ", 1-error))
error <- mean(results != final.test$Survived)
agefunction <- function(AgeData,classData,classMeanVector) {
for(i in 1:length(AgeData)) {
if(is.na(AgeData[i])) {
AgeData[i] <- classMeanVector[classData[i]]
}
}
return(AgeData)
}
#Logistic regression lecture (titanic kaggle)
df.train <- read.csv("/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/8 - Machine Learning with R/titanic_train.csv")
head(df.train)
#missing values
missmap(df.train, col= c("yellow","black"), legend = FALSE)
#survived
ggplot(df.train, aes(Survived)) + geom_bar()
#class
ggplot(df.train, aes(Pclass)) + geom_bar(aes(fill = factor(Pclass)), alpha = 0.5)
#sex
ggplot(df.train, aes(Sex)) + geom_bar() + aes(fill = factor(Sex), alpha = 0.5)
#age
ggplot(df.train, aes(Age)) + geom_histogram(fill = "blue", alpha = 0.5, bins=20)
#SibSp (siblins/spouses aboard)
ggplot(df.train, aes(SibSp)) + geom_bar(fill = "red", alpha = 0.5)
#fare
ggplot(df.train, aes(Fare)) + geom_histogram(color="black", fill="green", alpha=0.5)
#boxplot of age by class
ggplot(df.train, aes(Pclass, Age)) + geom_boxplot() + aes(group=Pclass,fill = factor(Pclass), alpha=0.5)
#as we have seen in the boxplot, the ages of classes are different from each other
#so, it makes sense to divide them to put different means in the N.A values
#going to use the function defined in the beginning to change ages
classMeanVector <- c(0,0,0)
for(i in 1:3){
classMeanVector[i] <- mean( filter(df.train, Pclass==i, Age!="NA")$Age )
}
df.train$Age <- agefunction(df.train$Age, df.train$Pclass,classMeanVector)
#removing colums i will not use
df.train <- select(df.train, -PassengerId, -Name, -Ticket, -Cabin)
df.train$Survived <- factor(df.train$Survived)
df.train$Pclass <- factor(df.train$Pclass)
df.train$Parch <- factor(df.train$Parch)
df.train$SibSp <- factor(df.train$SibSp)
str(df.train)
logiModel <- glm(formula = Survived ~ . , family= binomial(link = 'logit'),data=df.train)
summary(logiModel)
#splitting da+ta
set.seed(101)
split <- sample.split(df.train$Survived, SplitRatio= 0.7)
final.train <- subset(df.train, split==TRUE)
final.test <- subset(df.train, split==FALSE)
final.log.model <- glm(formula = Survived ~ ., family=binomial(link='logit'), data=final.train)
summary(final.log.model)
probabilities <- predict(final.log.model, newdata = final.test, type='response')
results <- ifelse(probabilities > 0.5,1,0)
error <- mean(results != final.test$Survived)
print(paste("Accuracy = ", 1-error))
#confusion matrix
table(final.test$Survived, probabilities>0.5)
employer_function <- function(type){
for(i in 1:length(type)) {
if (type[i]=="Without-pay" | type[i]=="Never-worked") {
type[i] <- "Unemployed"
}
if (type[i]=="Local-gov" | type[i]=="State-gov") {
type[i] <- "SL-gov"
}
if (type[i]=="Self-emp-inc" | type[i]=="Self-emp-not-inc") {
type[i] <- "Self-emp"
}
}
return(type)
}
marital_function <- function(marital_type) {
for(i in 1:length(marital_type)){
if (marital_type[i] =='Married-AF-spouse' |
marital_type[i] =='Married-civ-spouse' |
marital_type[i] =='Married-spouse-absent') {
marital_type[i] <- "Married"
}
if (marital_type[i] =='Divorced' |
marital_type[i] =='Separated' |
marital_type[i] =='Widowed') {
marital_type[i] <- "Not-married"
}
}
return(marital_type)
}
countries_function <- function(countries,Asia,N.A,Eu,LSA,Other) {
for(i in 1:length(countries)) {
if (countries[i] %in% Asia) {
countries[i] <- "Asia"
}
else if (countries[i] %in% N.A) {
countries[i] <- "North.America"
}
else if (countries[i] %in% Eu) {
countries[i] <- "Europe"
}
else if (countries[i] %in% LSA) {
countries[i] <- "L.S.America"
} else {
countries[i] <- "Other"
}
}
return(countries)
}
income_function<- function(salary) {
for(i in 1:length(salary)) {
if (salary[i] == '>50K') {
print(salary[i])
salary[i] <- 1
}
else {
salary[i] <- 0
}
}
return(salary)
}
adult <- read.csv('/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/adult_sal.csv')
head(adult)
#removing index
adult <- select(adult, -X)
str(adult)
summary(adult)
distinct(adult, type_employer)
summary(adult$type_employer)
#first, combining some employeers types using
#a function
adult$type_employer <- as.character(adult$type_employer)
adult$type_employer <- employer_function(adult$type_employer)
table(adult$type_employer)
#combining factors of marital colunn
adult$marital <- as.character(adult$marital)
adult$marital <- marital_function(adult$marital)
table(adult$marital)
#grouping countries in continents
adult$country <- as.character(adult$country)
table(adult$country)
levels(adult$country)
Asia <- c('China','Hong','India','Iran','Cambodia','Japan', 'Laos' ,
'Philippines' ,'Vietnam' ,'Taiwan', 'Thailand')
N.A <- c('Canada','United-States','Puerto-Rico')
Eu <- c('England' ,'France', 'Germany' ,'Greece','Holand-Netherlands','Hungary',
'Ireland','Italy','Poland','Portugal','Scotland','Yugoslavia')
LSA <- c('Columbia','Cuba','Dominican-Republic','Ecuador',
'El-Salvador','Guatemala','Haiti','Honduras',
'Mexico','Nicaragua','Outlying-US(Guam-USVI-etc)','Peru',
'Jamaica','Trinadad&Tobago')
Other <- c('South','?')
adult$country <- countries_function(adult$country, Asia,N.A,Eu,LSA,Other)
#renaming the column
adult <- adult %>% rename(region = country)
#transforming income column in a boolean column
#0 for <=50k and 1 for >50k
adult$income <- as.character(adult$income)
adult$income <- income_function(adult$income)
#converting '?' values in NA values to deal with missing data
adult[adult == '?'] <- NA
#making the changed columns factors again
adult$type_employer <- factor(adult$type_employer)
adult$country <- factor(adult$country)
adult$marital <- factor(adult$marital)
adult$income <- factor(adult$income)
str(adult)
#missing values
missmap(adult,col=c('yellow','black'))
#omit NA data
adult <- na.omit(adult)
str(adult)
#histogram of ages (colored by income)
ggplot(adult, aes(age)) + geom_histogram(color='black', binwidth=1) + aes(fill=(income))
#histogram of hours worked per week
ggplot(adult, aes(hr_per_week)) + geom_histogram(fill='black')
#barpl ot of region colored by income
ggplot(adult, aes(region)) + geom_bar(color='black') + aes(fill=income)
#model
set.seed(101)
#spliting train and test data
splitted <- sample.split(adult$income, SplitRatio = 0.7)
train.adult <- subset(adult, splitted==TRUE)
test.adult <- subset(adult, splitted==FALSE)
#training model
model <- glm(income~. , family=binomial(logit), data=train.adult)
summary(model)
#step model
step.model <- step(model)
summary(step.model)
#confusion matrix
probabilities <- predict(step.model, test.adult, type = 'response')
results <- ifelse(probabilities>0.5, 1,0)
table(test.adult$income,results)
#accuracy
accuracy <- (6372+1423)/(6372+548+872+1423)
accuracy
# We will apply the KNN approach
# to the Caravan data set, which
# is part of the ISLR library.
# This data set includes 85
# predictors that measure demographic
# characteristics for 5,822 individuals.
# The response variable is Purchase, which
# indicates whether or not a given individual
# purchases a Caravan insurance policy. In this
# data set, only 6% of people purchased caravan insurance.
library(ISLR)
#library with the KNN function
library(class)
head(Caravan)
str(Caravan)
summary(Caravan$Purchase)
#checking if there are any NA values
any(is.na(Caravan))
#standarizing variables (except Purchase)
purchase <- Caravan[,86]
std.Caravan <- scale(Caravan[,-86])
#splitting train and test data
splitted <- sample.split(std.Caravan[,85], SplitRatio = 0.7)
train.Caravan <- subset(std.Caravan, splitted==TRUE)
test.Caravan <- subset(std.Caravan, splitted==FALSE)
train.Purchase<- subset(purchase, splitted==TRUE)
test.Purchase <- subset(purchase, splitted==FALSE)
#model
predicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=1)
head(predicted.purchase)
#misclassification error rate
mean(test.Purchase != predicted.purchase)
#choosing K value (trying 1:20)
m.error <- NULL
for (i in 1:20) {
predicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=i)
m.error[i] <- mean(test.Purchase != predicted.purchase)
}
#plot of the errors
k.value <- 1:20
error.df <- data.frame(m.error,k.value)
error.df
ggplot(error.df, aes(k.value, m.error)) +geom_point() + geom_line(lty='dotted')
#as we can see in the graph, the optimum value for K is 12
preicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=12)
accuracy <- 1-m.error[12]
accuracy
#knn exercise
#iris dataset (from ISLR library)
head(iris)
str(iris)
#standardize
std.iris <- scale(iris[-5])
std.iris <- cbind(std.iris, iris[5])
head(std.iris)
#splitting train and test data
split <- sample.split(std.iris, SplitRatio = 0.7)
train.iris <- subset(std.iris, split==TRUE)
test.iris <- subset(std.iris, split==FALSE)
head(test.iris)
#model
predicted.species <- knn(train.iris[1:4], test.iris[1:4], train.iris$Species,k=1 )
#misclassification error
mean(predicted.species!=test.iris$Species)
#choosing k value
mError <- NULL
predicted.species <- NULL
for(i in 1:10) {
predicted.species <- knn(train.iris[1:4],test.iris[1:4],train.iris$Species,k=i)
mError[i] <- mean(predicted.species!=test.iris$Species)
}
#plot for errors/kvalues
kValues <- 1:10
m.Error <- data.frame(kValues, mError)
ggplot(m.Error, aes(kValues, mError)) + geom_point() + geom_line(lty="dotted")
#as we can see 3 is the optimum solution
#accuracy
1-m.Error$mError[3]
setwd("learning-ml/Problems/Classify Iris")
read.csv("data/Iris.csv")
read.csv("/home/jardel/learning-ml/Problems/Classify Iris/data/Iris.csv")
read.csv("learning-ml/Problems/Classify Iris/data/Iris.csv")
read.csv("/home/jardel/learning-ml/Problems/Classify Iris/data/Iris.csv")
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
read.csv("data/Iris.csv")
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
read.csv("data/Iris.csv")
library(class)
library(caTools)
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
read.csv("data/Iris.csv")
any(is.na(iris))
df <- read.csv("data/Iris.csv")
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
dim(train)
dim(test)
dim(df)
dim(train)
dim(test)
library(ggplot2)
library(class)
library(caTools)
# /* --------------------------------------------------------------------------------------- */
# /* Set working directory to this problem and read the csv file                             */
# /* --------------------------------------------------------------------------------------- */
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
df <- read.csv("data/Iris.csv")
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
dim(test)
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.9)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
dim(test)# EDA
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
dim(test)#EDA
# /* --------------------------------------------------------------------------------------- */
# /* Exploratory                                                                             */
# /* --------------------------------------------------------------------------------------- */
View(train)
# /* --------------------------------------------------------------------------------------- */
# /* Exploratory                                                                             */
# /* --------------------------------------------------------------------------------------- */
ggplot(train, aes(SepalLengthCm,PetalLengthCm)) + geom_point()
# /* --------------------------------------------------------------------------------------- */
# /* Exploratory                                                                             */
# /* --------------------------------------------------------------------------------------- */
ggplot(train, aes(SepalLengthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Width
ggplot(train, aes(SepalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Sepal Width
ggplot(train, aes(SepalLengthCm,SepalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Length
ggplot(train, aes(SepalLengthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Width
ggplot(train, aes(SepalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Sepal Width
ggplot(train, aes(SepalLengthCm,SepalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Length
ggplot(train, aes(SepalLengthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Width
ggplot(train, aes(SepalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Width vs Petal Length
ggplot(train, aes(SepalWidthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Width vs Petal width
ggplot(train, aes(SepalWidthCm,PetalWidthCm, color=Species)) + geom_point()
#Petal Length vs Petal Width
ggplot(train, aes(PetlalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Petal Length vs Petal Width
ggplot(train, aes(PetalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Sepal Width
ggplot(train, aes(SepalLengthCm,SepalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Length
ggplot(train, aes(SepalLengthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Width
ggplot(train, aes(SepalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Width vs Petal Length
ggplot(train, aes(SepalWidthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Width vs Petal width
ggplot(train, aes(SepalWidthCm,PetalWidthCm, color=Species)) + geom_point()
#Petal Length vs Petal Width
ggplot(train, aes(PetalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#k=1
predicted.species <- knn(train,test,train$Species, k=1)
#k=1
predicted.species <- knn(train[2:5],test[2:5],train$Species, k=1)
#misclassification error
m.error <- mean(predicted.species != test$Species)
m.error
1-m.error
m.error
m.error[i] <- mean(predicted.species != test$Species)
for (i in 1:10) {
predicted.species <- knn(train[2:5], test[2:5], train$Species, k=i)
m.error[i] <- mean(predicted.species != test$Species)
}
m.error
m.error <- data.table(k.values,m.error)
###### plotting misclassification errors with different k values
k.values <- 1:10
m.error <- data.table(k.values,m.error)
m.error
ggplot(m.error,aes(k.values,m.error)) + geom_point() + geom_line()
predicted.species <- knn(train[2:5], test[2:5], train$Species, k=3)
m.error <- mean(predicted.species != test$Species)
m.error
accuracy <- mean(predicted.species == test$Species)
accuracy
#we can see in the plot that 3 is the best value
predicted.species <- knn(train[2:5], test[2:5], train$Species, k=3)
accuracy <- mean(predicted.species == test$Species)
accuracy
## Iris Species Dataset
### Description (from Kaggle.com)
The Iris dataset was used in R.A. Fisher's classic 1936 paper,  [The Use of Multiple Measurements in Taxonomic Problems](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf), and can also be found on the  [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/).
It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.
The columns in this dataset are:
-   Id
-   SepalLengthCm
-   SepalWidthCm
-   PetalLengthCm
-   PetalWidthCm
-   Species
library(ggplot2)
library(class)
library(caTools)
# /* --------------------------------------------------------------------------------------- */
# /* Set working directory to this problem and read the csv file                             */
# /* --------------------------------------------------------------------------------------- */
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
df <- read.csv("data/Iris.csv")
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
#Sepal Length vs Sepal Width
ggplot(train, aes(SepalLengthCm,SepalWidthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Length
ggplot(train, aes(SepalLengthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Length vs Petal Width
ggplot(train, aes(SepalLengthCm,PetalWidthCm, color=Species)) + geom_point()
#Sepal Width vs Petal Length
ggplot(train, aes(SepalWidthCm,PetalLengthCm, color=Species)) + geom_point()
#Sepal Width vs Petal width
ggplot(train, aes(SepalWidthCm,PetalWidthCm, color=Species)) + geom_point()
#Petal Length vs Petal Width
ggplot(train, aes(PetalLengthCm,PetalWidthCm, color=Species)) + geom_point()
###### k=1
predicted.species <- knn(train[2:5],test[2:5],train$Species, k=1)
#misclassification error
m.error <- mean(predicted.species != test$Species)
m.error
###### choosing k value (1~10)
predicted.species <- NULL
m.error <- NULL
for (i in 1:10) {
predicted.species <- knn(train[2:5], test[2:5], train$Species, k=i)
m.error[i] <- mean(predicted.species != test$Species)
}
###### plotting misclassification errors with different k values
k.values <- 1:10
m.error <- data.table(k.values,m.error)
ggplot(m.error,aes(k.values,m.error)) + geom_point() + geom_line()
m.error <- data.table(k.values,m.error)
###### plotting misclassification errors with different k values
k.values <- 1:10
m.error <- data.table(k.values,m.error)
m.error <- data.frame(k.values,m.error)
m.error
ggplot(m.error,aes(k.values,m.error)) + geom_point() + geom_line()
#we can see in the plot that 3 is the best value
predicted.species <- knn(train[2:5], test[2:5], train$Species, k=3)
accuracy <- mean(predicted.species == test$Species)
accuracy
accuracy
