'Ireland','Italy','Poland','Portugal','Scotland','Yugoslavia')
LSA <- c('Columbia','Cuba','Dominican-Republic','Ecuador',
'El-Salvador','Guatemala','Haiti','Honduras',
'Mexico','Nicaragua','Outlying-US(Guam-USVI-etc)','Peru',
'Jamaica','Trinadad&Tobago')
Other <- c('South','?')
adult$country <- countries_function(adult$country, Asia,N.A,Eu,LSA,Other)
#renaming the column
adult <- adult %>% rename(region = country)
#transforming income column in a boolean column
#0 for <=50k and 1 for >50k
adult$income <- as.character(adult$income)
adult$income <- income_function(adult$income)
#converting '?' values in NA values to deal with missing data
adult[adult == '?'] <- NA
#making the changed columns factors again
adult$type_employer <- factor(adult$type_employer)
adult$region <- factor(adult$region)
adult$marital <- factor(adult$marital)
adult$income <- factor(adult$income)
str(adult)
#missing values
missmap(adult,col=c('yellow','black'))
#omit NA data
adult <- na.omit(adult)
#exploratory data analysis
str(adult)
#histogram of ages (colored by income)
ggplot(adult, aes(age)) + geom_histogram(color='black', binwidth=1) + aes(fill=(income))
#histogram of hours worked per week
ggplot(adult, aes(hr_per_week)) + geom_histogram(fill='black')
#barpl ot of region colored by income
ggplot(adult, aes(region)) + geom_bar(color='black') + aes(fill=income)
#model
#spliting train and test data
splitted <- sample.split(adult$income, SplitRatio = 0.7)
train.adult <- subset(adult, splitted==TRUE)
test.adult <- subset(adult, splitted==FALSE)
#training model
model <- glm(income~. , family=binomial(logit), data=train.adult)
summary(model)
#step model
step.model <- step(model)
summary(step.model)
#confusion matrix
probabilities <- predict(step.model, test.adult, type = 'response')
results <- ifelse(probabilities>0.5, 1,0)
table(test.adult$income,results)
#accuracy
accuracy <- (6417+1385)/(6417+503+910+1385)
accuracy
####################################
# KNN lecture
#library of the Caravan dataset
# We will apply the KNN approach
# to the Caravan data set, which
# is part of the ISLR library.
# This data set includes 85
# predictors that measure demographic
# characteristics for 5,822 individuals.
# The response variable is Purchase, which
# indicates whether or not a given individual
# purchases a Caravan insurance policy. In this
# data set, only 6% of people purchased caravan insurance.
library(ISLR)
#library with the KNN function
library(class)
3head(Caravan)
str(Caravan)
summary(Caravan$Purchase)
#checking if there are any NA values
any(is.na(Caravan))
#standarizing variables (except Purchase)
purchase <- Caravan[,86]
std.Caravan <- scale(Caravan[,-86])
#splitting train and test data
splitted <- sample.split(std.Caravan[,85], SplitRatio = 0.7)
train.Caravan <- subset(std.Caravan, splitted==TRUE)
test.Caravan <- subset(std.Caravan, splitted==FALSE)
train.Purchase<- subset(purchase, splitted==TRUE)
test.Purchase <- subset(purchase, splitted==FALSE)
#model
predicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=1)
head(predicted.purchase)
#misclassification error rate
mean(test.Purchase != predicted.purchase)
#choosing K value (trying 1:20)
m.error <- NULL
for (i in 1:20) {
predicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=i)
m.error[i] <- mean(test.Purchase != predicted.purchase)
}
#plot of the errors
k.value <- 1:20
error.df <- data.frame(m.error,k.value)
error.df
ggplot(error.df, aes(k.value, m.error)) +geom_point() + geom_line(lty='dotted')
#as we can see in the graph, the optimum value for K is 12
preicted.purchase <- knn(train.Caravan, test.Caravan, train.Purchase, k=12)
accuracy <- 1-m.error[12]
accuracy
######### trees / random forest lecture
#tree
library(rpart)
library(rpart.plot)
str(kyphosis)
head(kyphosis)
tree <- rpart(Kyphosis ~., method = 'class', data = kyphosis)
printcp(tree)
plot(tree,uniforme=TRUE, main= "Title")
text(tree, use.n=TRUE,all=TRUE)
#better plot
prp(tree)
#random forest
library(randomForest)
model <- randomForest(Kyphosis ~., data = kyphosis)
print(model)
importance(model)
######## trees/random forest exercise
library(ISLR)
library(ggplot2)
library(randomForest)
library(dplyr)
library(caTools)
library(rpart)
library(rpart.plot)
df <- College
head(df)
str(df)
View(df)
#eda (fazer mais qd postar no git)
ggplot(df, aes(Grad.Rate,fill=Private)) +geom_histogram(color='black')
#college with a grad rate above 100%
x<- subset(df, Grad.Rate>100)
#change to 100
df['Cazenovia College', "Grad.Rate"] <- 100
#split data
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
#decision tree
tree <- rpart(Private ~., data = train, method='class')
tree.pred <- predict(tree,test)
head(tree.pred)
results <- NULL
for (i in 1:length(tree.pred[,2])) {
if (tree.pred[i,2] >= 0.5) {
results[i] <- 'Yes'
} else {
results[i] <- 'No'
}
}
tree.pred <- cbind(tree.pred, results)
tree.pred <- data.frame(tree.pred)
table(tree.pred$results, test$Private)
(63+174)/(13+9+63+174)
prp(tree)
#random forest
rf.model <- randomForest(Private~., data=train,importance=TRUE)
rf.model$confusion
rf.model$importance
rf.pred <- predict(rf.model, test)
table(rf.pred, test$Private)
(62+179)/(62+8+10+179)
######## support vector machines
#lecture
library(e1071)
head(iris)
model <- svm(Species ~., data=iris)
summary(model)
predicted.values <- predict(model,iris[1:4])
table(predicted.values,iris[,5])
# Tune for combos of gamma 0.5,1,2
# and costs 1/10 , 10 , 100
tune.results <- tune(svm,train.x=iris[1:4],train.y=iris[,5],kernel='radial',
ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
summary(tune.results)
tuned.svm <- svm(Species ~ ., data=iris, kernel="radial", cost=1, gamma=0.5)
summary(tuned.svm)
tuned.predicted.values <- predict(tuned.svm,iris[1:4])
table(tuned.predicted.values,iris[,5])
#svm project
loans <-read.csv("/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/loan_data.csv")
head(loans)
str(loans)
summary(loans)
#converting columns
loans$inq.last.6mths <- factor(loans$inq.last.6mths)
loans$delinq.2yrs <- factor(loans$delinq.2yrs)
loans$pub.rec <- factor(loans$pub.rec)
loans$not.fully.paid <- factor(loans$not.fully.paid)
loans$credit.policy <- factor(loans$credit.policy)
#eda
#fico scores colored by not fully paid
ggplot(loans, aes(fico,fill=not.fully.paid))+geom_histogram(color = 'black')
# purpose counts, colored by not.fully.paid
ggplot(loans,aes(purpose,fill=not.fully.paid)) + geom_bar(color ='black',position='dodge')
#fico vs int.rate
ggplot(loans,aes(int.rate,fico)) +geom_point()
#split data
set.seed(101)
split <- sample.split(loans, SplitRatio = 0.7)
train <- subset(loans, split==TRUE)
test <- subset(loans, split==FALSE)
#model
model <- svm(not.fully.paid ~. , data=train)
summary(model)
predicted <- predict(model, test[1:13])
table(predicted, test$not.fully.paid)
#tuning
tune.results <- tune(svm,train.x=not.fully.paid~., data=train,kernel='radial',
ranges=list(cost=c(1,10), gamma=c(0.1,1)))
model <- svm(not.fully.paid ~ .,data=train,cost=10,gamma = 0.1)
predicted <- predict(model,test[1:13])
table(predicted,test$not.fully.paid)
############## k means clustering
#lecture
head(iris)
#eda
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
#clustering
set.seed(101)
#as we already know how many clusters to expect
irisCluster <- kmeans(iris[,1:4],3,nstart=20)
irisCluster
#as we can see in this table, looks like we correct grouped the setosa
#versicolor and virginica get a little noise
table(irisCluster$cluster, iris$Species)
#library for clusters plot
library(cluster)
clusplot(iris,irisCluster$cluster, color=TRUE,shade=TRUE,labels=0,lines=0,)
#exercise
df1 <- read.csv('/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/winequality-red.csv', sep = ';')
df2 <- read.csv('/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/winequality-white.csv', sep = ';')
#creating a label (red or white) column
df1$label <- 'red'
df2$label <- 'white'
head(df1)
head(df2)
#combining them into a single df
wine <- rbind(df1,df2)
str(wine)
#eda
#residual sugar (colored by wine)
ggplot(wine, aes(residual.sugar, fill=label)) + geom_histogram(bins=50,color='black')
#citric acid (colored by wine)
ggplot(wine, aes(citric.acid, fill=label)) + geom_histogram(color='black', bins=50)
#alcohol (colored by wine)
ggplot(wine, aes(alcohol, fill=label)) + geom_histogram(color='black',bins=50)
#residual sugar vs citric acid (colored by wine)
ggplot(wine, aes(residual.sugar,citric.acid,color=label)) + geom_point(alpha=0.1)
#volatile acidity vs residual sugar (colored by wine)
ggplot(wine, aes(volatile.acidity,residual.sugar, color=label)) +geom_point(alpha=0.1)
#model
wine.cluster <- kmeans(wine[1:12],2)
wine.cluster$centers
table( wine$label,wine.cluster$cluster)
#lecture
library(MASS)
set.seed(101)
df <- Boston
str(df)
str(df)
summary(df)
head(df)
any(is.na(df))
install.packages('neuralnet')
library(neuralnet)
#normalize data
maxs <- apply(data,2,max)
min <- apply(data,2,min)
#normalize data
maxs <- apply(data, 2, max)
#normalize data
maxs <- apply(data, 2, max)
min <- apply(data, 2, min)
#normalize data
maxs <- apply(data, 2, max)
min <- apply(data, 2, min)
#lecture
library(MASS)
library(neuralnet)
set.seed(101)
df <- Boston
str(df)
summary(df)
head(df)
any(is.na(df))
#normalize data
maxs <- apply(data, 2, max)
min <- apply(data, 2, min)
#normalize data
maxs <- max(df)
maxs
#normalize data
maxs <- apply(data, 2, max)
min <- apply(data, 2, min)
#normalize data
maxs <- sapply(data, 2, max)
#normalize data
maxs <- apply(data, 2, max)
#normalize data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
#normalize data
maxs <- apply(data, 2, max)
mins <- apply(data, 2, min)
#normalize data
scaled <- as.data.frame(scale(df))
head(scaled)
#split data
split <- sample.split(df, SplitRatio = 0.7)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
train
dim(train)
dim(test)
library(neuralnet)
#model
n <- names(train)
f <- as.formula(paste("medv ~", paste(n, collapse = ' + ')))
f
f <- as.formula(paste("medv ~", paste(n[n!="medv"], collapse = ' + ')))
f
help(ne"neuralnet"neuralnet")
help(neuralnet)
nn <- neuralnet(f, data=train, hidden=c(5,3), linear.output = TRUE)
plot(nn)
nn
plot(nn)
set.seed(101)
df <- Boston
str(df)
summary(df)
head(df)
any(is.na(df))
#normalize data
scaled <- as.data.frame(scale(df))
head(scaled)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
#model
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[n!="medv"], collapse = ' + ')))
f
nn <- neuralnet(f, data=train, hidden=c(5,3), linear.output = TRUE)
plot(nn)
#predictions
predictions <- compute(nn, test[1:13])
predictions
str(predictions)
#converting back to non-scaled values
predictions <- predictions$net.result
#predictions
predictions <- compute(nn, test[1:13])
#converting back to non-scaled values
predictions$net.result
#lecture
library(MASS)
library(neuralnet)
set.seed(101)
df <- Boston
str(df)
summary(df)
head(df)
any(is.na(df))
#normalize data
scaled <- scale(df)
b <- attr(scaled, "scaled:scale")
a <- attr(scaled, "scaled:center")
head(scaled)
scaled <- as.data.frame(scaled)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
#model
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[n!="medv"], collapse = ' + ')))
nn <- neuralnet(f, data=train, hidden=c(5,3), linear.output = TRUE)
#plot nn
plot(nn)
#predictions
predictions <- compute(nn, test[1:13])
#normalize data
scaled0 <- scale(df)
b <- attr(scaled, "scaled:scale")
a <- attr(scaled, "scaled:center")
#normalize data
scaled0 <- scale(df)
b <- attr(scaled0, "scaled:scale")
a <- attr(scaled0, "scaled:center")
head(scaled)
head(scaled0)
scaled <- as.data.frame(scaled0)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
#model
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[n!="medv"], collapse = ' + ')))
nn <- neuralnet(f, data=train, hidden=c(5,3), linear.output = TRUE)
#plot nn
plot(nn)
#predictions
predictions <- compute(nn, test[1:13])
b <- attr(scaled0, "scaled:scale")
a <- attr(scaled0, "scaled:center")
predictions <- predictions$net.result * rep(b, each = nrow(scaled0)) +rep(a, each=nrow(scaled0))
predictions <- predictions$net.result * rep(b, each = nrow(train)) +rep(a, each=nrow(train))
#normalize data
maxs <- apply(df, 2, max)
mins <- apply(df, 2, min0
maxs
mins <- apply(df, 2, min)
maxs
mins
mins <- apply(data, 2, min)
#normalize data
maxs <- apply(df, 2, max)
mins <- apply(df, 2, min)
scaled <- as.data.frame(scale(df,center = mins,scale=maxs-mins))
head(scaled)
#split data
split <- sample.split(scaled, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
#model
n <- names(train)
f <- as.formula(paste("medv ~", paste(n[n!="medv"], collapse = ' + ')))
nn <- neuralnet(f, data=train, hidden=c(5,3), linear.output = TRUE)
#plot nn
plot(nn)
#predictions
predictions <- compute(nn, test[1:13])
#converting back to non-scaled values
predictions <- predictions$net.result *(max(df$medv)-min(df$medv)) + min(df$medv)
head(predictions)
test.r <- test$medv * (max(df$medv)-min(df$medv)) + min(df$medv)
#Mean squared error
MSE <- sum((test.r-predictions)^2)/nrow(test)
MSE
#visualize error
error.df <- data.frame(test.r, predictions)
library(ggplot2)
library(class)
library(caTools)
# /* --------------------------------------------------------------------------------------- */
# /* Set working directory to this problem and read the csv file                             */
# /* --------------------------------------------------------------------------------------- */
setwd("/home/jardel/learning-ml/Problems/Classify Iris")
df <- read.csv("data/Iris.csv")
# /* --------------------------------------------------------------------------------------- */
# /* Split dataset into a training and a test set                                            */
# /* --------------------------------------------------------------------------------------- */
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE)
test <- subset(df, split==FALSE)
#Sepal Length vs Sepal Width
ggplot(train, aes(SepalLengthCm,SepalWidthCm, color=Species)) + geom_point()
#visualize error
error.df <- data.frame(test.r, predictions)
#visualize error
error.df <- data.frame(test.r, predictions)
#visualize error
error.df <- data.frame(test.r, predictions)
ggplot(error.df, aes(test.r,predictions))+geom_point()
ggplot(error.df, aes(test.r,predictions))+geom_point()+stat_smooth()
ggplot(error.df, aes(test.r,predictions))+geom_point()+geom_smooth()
df <- read.csv('/home/jardel/Dropbox/R-Course-HTML-Notes/R-for-Data-Science-and-Machine-Learning/Training Exercises/Machine Learning Projects/CSV files for ML Projects/bank_note_data.csv')
head(df)
head(df)
str(df)
split <- sample.split(df,SplitRatio = 0.7)
test <- subset(df,split==F)
train <- subset(df,split==T)
str(train)
#model
f <- names(train)
f
#model
x <- names(train)
#model
xn <- names(train)
#model
n <- names(train)
f <- as.formula(paste("Class ~", paste(n[n!="Class"], collapse = ' + ')))
nn <- neuralnet(f,hidden=10,linear.output = FALSE)
nn <- neuralnet(f,hidden=10,linear.output = FALSE,data=train)
plot(nn)
#predictions
predictions <- compute(nn, test[1:4])
head(predictions)
head(predictions$net.result)
#predictions
predictions <- compute(nn, test[,1:4])
head(predictions$net.result)
predictions$net.result
n
f
predictions <- round(predictions$net.result)
predictions
table(predictions,test$Class)
